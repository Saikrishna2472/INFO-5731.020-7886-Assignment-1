{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saikrishna2472/INFO-5731.020-7886-Assignment-1/blob/main/Paleru_Jai_Sai_Krishna_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "A text classification about a task like analyzing customer reviews for products. The goal is to figure out whether people are feeling positive, negative or neutral about what they purchased. This kind of analysis can be valuable for businesses as it can help them understand how a customer feels and where they need to make improvements.\n",
        "\n",
        "Features That Would Help Us Build the Model\n",
        "\n",
        "Term Frequency-Inverse Document Frequency\n",
        "This is a way to identify how important a word is in a particular review compared to all reviews. It highlights words that stand out and aren’t just commonly used everywhere.\n",
        "\n",
        "Use: By focusing on unique words that capture customer feelings, we can better understand what really matters in a review.\n",
        "\n",
        "Sentiment Lexicons\n",
        "These are lists of words that are known to convey specific feelings like “amazing” for positive sentiments or “horrible” for negative ones.\n",
        "\n",
        "Use: Using these lists gives us a quick way to gauge the sentiment of a review. If a review is filled with positive words we can confidently classify it as positive.\n",
        "\n",
        "N-grams\n",
        "These are groups of words that appear together in a review. A unigram is a single word while a bigram is a pair of words and a trigram is a group of three.\n",
        "\n",
        "Use: N-grams help capture the context better. For example “not great” as a bigram conveys a different meaning than just looking at the words individually.\n",
        "\n",
        "Part-of-Speech (POS) Tags\n",
        "This involves identifying whether a word is a noun, verb, adjective, etc.\n",
        "\n",
        "Use: Adjectives and adverbs often carry emotional weight. Knowing how many descriptive words are used can give us clues about the overall sentiment.\n",
        "\n",
        "Length of Review\n",
        "This is simply the number of words or characters in a review.\n",
        "\n",
        "Use: Sometimes, longer reviews can indicate more nuanced feelings, while shorter ones may express strong opinions. Understanding the length can help us gauge sentiment intensity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240c4370-80bb-4e3b-c9e6-c708792de290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        reviews  review_length  pos_count  \\\n",
            "0           This product is amazing! I love it.              7          2   \n",
            "1  Terrible experience. It broke after one use.              7          0   \n",
            "2  It's okay, nothing special but does the job.              8          0   \n",
            "3          Fantastic quality! Highly recommend.              4          3   \n",
            "4   Worst purchase ever. I'm very disappointed.              6          0   \n",
            "\n",
            "   neg_count                                            bigrams  \\\n",
            "0          0  [(this, product), (product, is), (is, amazing)...   \n",
            "1          2  [(terrible, experience), (experience, .), (., ...   \n",
            "2          0  [(it, 's), ('s, okay), (okay, ,), (,, nothing)...   \n",
            "3          0  [(fantastic, quality), (quality, !), (!, highl...   \n",
            "4          2  [(worst, purchase), (purchase, ever), (ever, ....   \n",
            "\n",
            "                                            pos_tags  \n",
            "0  {'DT': 1, 'NN': 1, 'VBZ': 1, 'JJ': 1, '.': 2, ...  \n",
            "1  {'JJ': 1, 'NN': 2, '.': 2, 'PRP': 1, 'VBD': 1,...  \n",
            "2  {'PRP': 1, 'VBZ': 2, 'JJ': 2, ',': 1, 'NN': 2,...  \n",
            "3               {'JJ': 1, 'NN': 2, '.': 2, 'NNP': 1}  \n",
            "4  {'NNP': 1, 'NN': 1, 'RB': 2, '.': 2, 'PRP': 1,...  \n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk import ngrams\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Sample product reviews\n",
        "data = {\n",
        "    'reviews': [\n",
        "        \"This product is amazing! I love it.\",\n",
        "        \"Terrible experience. It broke after one use.\",\n",
        "        \"It's okay, nothing special but does the job.\",\n",
        "        \"Fantastic quality! Highly recommend.\",\n",
        "        \"Worst purchase ever. I'm very disappointed.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# 1. TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['reviews'])\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# 2. Sentiment Lexicon (for demonstration, we'll create a simple one)\n",
        "positive_words = set([\"amazing\", \"love\", \"fantastic\", \"highly\", \"recommend\"])\n",
        "negative_words = set([\"terrible\", \"broke\", \"worst\", \"disappointed\", \"bad\"])\n",
        "\n",
        "# Function to calculate sentiment lexicon features\n",
        "def sentiment_lexicon_features(review):\n",
        "    words = word_tokenize(review.lower())\n",
        "    pos_count = len([word for word in words if word in positive_words])\n",
        "    neg_count = len([word for word in words if word in negative_words])\n",
        "    return pos_count, neg_count\n",
        "\n",
        "# 3. N-grams Extraction\n",
        "def extract_ngrams(review, n=2):\n",
        "    words = word_tokenize(review.lower())\n",
        "    return list(ngrams(words, n))\n",
        "\n",
        "# 4. Part-of-Speech Tagging\n",
        "def pos_features(review):\n",
        "    words = word_tokenize(review)\n",
        "    pos_tags = pos_tag(words)\n",
        "    return Counter(tag for word, tag in pos_tags)\n",
        "\n",
        "# 5. Length of Review\n",
        "df['review_length'] = df['reviews'].apply(lambda x: len(x.split()))\n",
        "\n",
        "# Extract features and store them in the DataFrame\n",
        "df['pos_count'], df['neg_count'] = zip(*df['reviews'].apply(sentiment_lexicon_features))\n",
        "df['bigrams'] = df['reviews'].apply(lambda x: extract_ngrams(x, 2))\n",
        "df['pos_tags'] = df['reviews'].apply(pos_features)\n",
        "\n",
        "# Display the DataFrame with features\n",
        "print(df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9ab683-cbaf-43c2-a5dd-95ed4cf4b6ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         feature  chi2_score\n",
            "14       nothing    1.465633\n",
            "15          okay    1.465633\n",
            "3            but    1.465633\n",
            "23           the    1.465633\n",
            "5           does    1.465633\n",
            "21       special    1.465633\n",
            "12           job    1.465633\n",
            "20     recommend    0.750000\n",
            "8      fantastic    0.750000\n",
            "9         highly    0.750000\n",
            "19       quality    0.750000\n",
            "26          very    0.670820\n",
            "18      purchase    0.670820\n",
            "27         worst    0.670820\n",
            "6           ever    0.670820\n",
            "4   disappointed    0.670820\n",
            "1        amazing    0.642617\n",
            "13          love    0.642617\n",
            "17       product    0.642617\n",
            "10            is    0.642617\n",
            "24          this    0.642617\n",
            "16           one    0.590692\n",
            "7     experience    0.590692\n",
            "22      terrible    0.590692\n",
            "25           use    0.590692\n",
            "2          broke    0.590692\n",
            "0          after    0.590692\n",
            "11            it    0.059160\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Sample product reviews and labels\n",
        "data = {\n",
        "    'reviews': [\n",
        "        \"This product is amazing! I love it.\",\n",
        "        \"Terrible experience. It broke after one use.\",\n",
        "        \"It's okay, nothing special but does the job.\",\n",
        "        \"Fantastic quality! Highly recommend.\",\n",
        "        \"Worst purchase ever. I'm very disappointed.\"\n",
        "    ],\n",
        "    'sentiment': ['positive', 'negative', 'neutral', 'positive', 'negative']\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Convert sentiments to numerical values\n",
        "le = LabelEncoder()\n",
        "df['sentiment_encoded'] = le.fit_transform(df['sentiment'])\n",
        "\n",
        "# 1. TF-IDF Feature Extraction\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['reviews'])\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# 2. Feature Selection using Chi-Squared\n",
        "chi2_values, p_values = chi2(tfidf_matrix, df['sentiment_encoded'])\n",
        "\n",
        "# Create a DataFrame to hold feature names and their Chi-Squared scores\n",
        "chi2_df = pd.DataFrame({\n",
        "    'feature': tfidf_feature_names,\n",
        "    'chi2_score': chi2_values\n",
        "})\n",
        "\n",
        "# Rank the features by their Chi-Squared scores\n",
        "chi2_df = chi2_df.sort_values(by='chi2_score', ascending=False)\n",
        "\n",
        "# Display the ranked features\n",
        "print(chi2_df)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f11630b-f435-40fb-a289-5a08b4377c94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        reviews  similarity_score\n",
            "0           This product is amazing! I love it.          0.932473\n",
            "3          Fantastic quality! Highly recommend.          0.662408\n",
            "4   Worst purchase ever. I'm very disappointed.          0.634917\n",
            "2  It's okay, nothing special but does the job.          0.585165\n",
            "1  Terrible experience. It broke after one use.          0.509220\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Sample product reviews\n",
        "data = {\n",
        "    'reviews': [\n",
        "        \"This product is amazing! I love it.\",\n",
        "        \"Terrible experience. It broke after one use.\",\n",
        "        \"It's okay, nothing special but does the job.\",\n",
        "        \"Fantastic quality! Highly recommend.\",\n",
        "        \"Worst purchase ever. I'm very disappointed.\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define a query\n",
        "query = \"I really love this product, it's fantastic!\"\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Function to get BERT embeddings\n",
        "def get_bert_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # We take the mean of the last hidden state as the embedding\n",
        "    return outputs.last_hidden_state.mean(dim=1).numpy().flatten()  # Flatten to 1D\n",
        "\n",
        "# Get embeddings for the reviews and the query\n",
        "review_embeddings = np.array([get_bert_embedding(review) for review in df['reviews']])\n",
        "query_embedding = get_bert_embedding(query)\n",
        "\n",
        "# Calculate cosine similarity\n",
        "similarity_scores = cosine_similarity(query_embedding.reshape(1, -1), review_embeddings).flatten()\n",
        "\n",
        "# Add similarity scores to the DataFrame\n",
        "df['similarity_score'] = similarity_scores\n",
        "\n",
        "# Rank the reviews by similarity score in descending order\n",
        "df_ranked = df.sort_values(by='similarity_score', ascending=False)\n",
        "\n",
        "# Display the ranked reviews along with their similarity scores\n",
        "print(df_ranked[['reviews', 'similarity_score']])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "Working on extracting features from text data was a valuable experience. Here are some key takeaways:\n",
        "\n",
        "Feature Representation: I learned how to represent text using techniques like TF-IDF and BERT embeddings. TF-IDF shows the importance of words in documents while BERT provides a deeper understanding of meaning.\n",
        "\n",
        "Feature Selection: Understanding feature selection methods, especially using Chi-Squared tests, helped me see how important it is to choose relevant features for improving model performance.\n",
        "\n",
        "Similarity Measures: Calculating cosine similarity for ranking texts based on a query was particularly helpful. It showed how embeddings can be used for tasks beyond just classification.\n",
        "\n",
        "Challenges Encountered\n",
        "Technical Issues: I faced challenges with the shape of embeddings when calculating cosine similarity. This highlighted the importance of ensuring data shapes match for calculations.\n",
        "\n",
        "Model Complexity: Using BERT required a better understanding of how transformer models work, which was initially tricky.\n",
        "\n",
        "Library Management: Managing different library versions and ensuring compatibility added some complexity, especially for those new to Python.\n",
        "\n",
        "Relevance to Your Field of Study\n",
        "This exercise is very relevant to natural language processing (NLP):\n",
        "\n",
        "Feature Extraction: Learning feature extraction techniques is essential, as they greatly affect how well models perform in tasks like sentiment analysis.\n",
        "\n",
        "Deep Learning: Using BERT shows the shift toward deep learning in NLP, capturing language context better than traditional methods.\n",
        "\n",
        "Real-World Applications: These skills are applicable in many real-world scenarios, such as chatbots and recommendation systems, which rely heavily on text processing.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "# Colab link: https://colab.research.google.com/drive/1IoW79cs9yBviW9Q_OlywwCwlbpgA_Tz-?hl=en#scrollTo=CAq0DZWAhU9m"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}